{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5064b1fa-7e1a-45f5-a5ab-41dc189e9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install segmentation_models_pytorch-*.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14816054-3e25-4e51-9ffc-333564ffb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/bohr/train-4gug/v2')\n",
    "from dataloader import load_data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.encoder1 = self.conv_block(in_channels, 32)\n",
    "        self.encoder2 = self.conv_block(32, 64)\n",
    "        self.encoder3 = self.conv_block(64, 128)\n",
    "        self.encoder4 = self.conv_block(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.conv_block(512, 256)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(256, 128)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(128, 64)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(64, 32)\n",
    "\n",
    "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "\n",
    "        # Decoder path\n",
    "        dec4 = self.up4(bottleneck)\n",
    "        enc4_cropped = enc4[:, :, :dec4.size(2), :dec4.size(3)]\n",
    "        dec4 = self.decoder4(torch.cat([dec4, enc4_cropped], dim=1))\n",
    "\n",
    "        dec3 = self.up3(dec4)\n",
    "        enc3_cropped = enc3[:, :, :dec3.size(2), :dec3.size(3)]\n",
    "        dec3 = self.decoder3(torch.cat([dec3, enc3_cropped], dim=1))\n",
    "\n",
    "        dec2 = self.up2(dec3)\n",
    "        enc2_cropped = enc2[:, :, :dec2.size(2), :dec2.size(3)]\n",
    "        dec2 = self.decoder2(torch.cat([dec2, enc2_cropped], dim=1))\n",
    "\n",
    "        dec1 = self.up1(dec2)\n",
    "        enc1_cropped = enc1[:, :, :dec1.size(2), :dec1.size(3)]\n",
    "        dec1 = self.decoder1(torch.cat([dec1, enc1_cropped], dim=1))\n",
    "\n",
    "        out = self.final(dec1)\n",
    "        out = F.pad(out, (0, 181 - out.shape[-1], 0, 50 - out.shape[-2]))\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "def train(model, train_loader, test_loader, optimizer, criterion, num_epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        train_weighted_acc = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.float()\n",
    "            labels = labels.float()\n",
    "            images = images.cuda() if torch.cuda.is_available() else images\n",
    "            labels = labels.cuda() if torch.cuda.is_available() else labels\n",
    "            \n",
    "            outputs = model(images)\n",
    "            acc = compute_weighted_accuracy(outputs, labels, threshold=0.5)\n",
    "            outputs = outputs.view(outputs.size(0), outputs.size(1), -1)  # [B, C, H*W]\n",
    "            labels = labels.view(labels.size(0), -1)  # [B, H*W]\n",
    "            labels = labels.long() \n",
    "            loss = criterion(outputs, labels)\n",
    "            train_weighted_acc += acc\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / batch_count\n",
    "        avg_train_acc = train_weighted_acc / batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_weighted_acc = 0.0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.cuda() if torch.cuda.is_available() else images\n",
    "                labels = labels.cuda() if torch.cuda.is_available() else labels\n",
    "                \n",
    "                outputs = model(images)\n",
    "                acc = compute_weighted_accuracy(outputs, labels, threshold=0.5)\n",
    "                outputs = outputs.view(outputs.size(0), outputs.size(1), -1)\n",
    "                labels = labels.view(labels.size(0), -1)\n",
    "                labels = labels.long() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_weighted_acc += acc\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batch_count += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batch_count\n",
    "        avg_val_acc = val_weighted_acc / val_batch_count\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if (epoch+1) % 2 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}')\n",
    "            print('weighted accuracy train:', avg_train_acc)\n",
    "            print('weighted accuracy val:', avg_val_acc)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.load(self.files[idx])  # [7, 50, 181]\n",
    "        inputs = tensor[:6].float()  # [6, 50, 181]\n",
    "        label = tensor[6].float()    # [50, 181]\n",
    "        sample = {'image': inputs, 'mask': label}\n",
    "\n",
    "        return sample['image'], sample['mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def load_data(base_path, batch_size=4, test_size=0.2, transform=None):\n",
    "    from glob import glob\n",
    "    all_files = glob(os.path.join(base_path, \"*.pt\"))\n",
    "    train_files, test_files = train_test_split(all_files, test_size=test_size, random_state=42)\n",
    "\n",
    "    train_dataset = RadarDataset(train_files, transform=transform)\n",
    "    test_dataset = RadarDataset(test_files, transform=None)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def compute_weighted_accuracy(preds, targets, threshold=0.5, target_weight=1500):\n",
    "    \"\"\"\n",
    "    preds: raw logits from the model (B, C, H, W)\n",
    "    targets: ground truth labels (B, H, W)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get predicted class (argmax for multi-class, softmax + threshold for binary)\n",
    "        probs = torch.softmax(preds, dim=1)\n",
    "        pred_class = (probs[:, 1, :, :] > threshold).long()\n",
    "        \n",
    "        # Ensure targets is binary\n",
    "        true_class = (targets == 1).long()\n",
    "\n",
    "        # Calculate correct predictions\n",
    "        correct_target = ((pred_class == 1) & (true_class == 1)).sum()\n",
    "        correct_background = ((pred_class == 0) & (true_class == 0)).sum()\n",
    "\n",
    "        # Total number of target and background points\n",
    "        total_target = (true_class == 1).sum()\n",
    "        total_background = (true_class == 0).sum()\n",
    "\n",
    "        # Weighted score\n",
    "        correct_score = correct_background + target_weight * correct_target\n",
    "        total_score = total_background + target_weight * total_target\n",
    "\n",
    "        return (correct_score / total_score).item() if total_score > 0 else 0.0\n",
    "    \n",
    "data_path = '/bohr/train-4gug/v2/training_set'\n",
    "\n",
    "train_loader, test_loader = load_data(\n",
    "    base_path=data_path,\n",
    "    batch_size=4,  \n",
    "    test_size=0.2,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "class_weights = [1., 1500.]\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "pos_weight = torch.tensor([1500.0]).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) \n",
    "\n",
    "train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff5d0b-e198-45ac-bd85-5e19f949fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your model code (including the necessary imported modules, such as torch and torch.nn) below to generate a model structure file that can be easily loaded by the grading platform\n",
    "model_code ='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.encoder1 = self.conv_block(in_channels, 32)\n",
    "        self.encoder2 = self.conv_block(32, 64)\n",
    "        self.encoder3 = self.conv_block(64, 128)\n",
    "        self.encoder4 = self.conv_block(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.conv_block(512, 256)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(256, 128)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(128, 64)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(64, 32)\n",
    "\n",
    "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "\n",
    "        # Decoder path\n",
    "        dec4 = self.up4(bottleneck)\n",
    "        enc4_cropped = enc4[:, :, :dec4.size(2), :dec4.size(3)]\n",
    "        dec4 = self.decoder4(torch.cat([dec4, enc4_cropped], dim=1))\n",
    "\n",
    "        dec3 = self.up3(dec4)\n",
    "        enc3_cropped = enc3[:, :, :dec3.size(2), :dec3.size(3)]\n",
    "        dec3 = self.decoder3(torch.cat([dec3, enc3_cropped], dim=1))\n",
    "\n",
    "        dec2 = self.up2(dec3)\n",
    "        enc2_cropped = enc2[:, :, :dec2.size(2), :dec2.size(3)]\n",
    "        dec2 = self.decoder2(torch.cat([dec2, enc2_cropped], dim=1))\n",
    "\n",
    "        dec1 = self.up1(dec2)\n",
    "        enc1_cropped = enc1[:, :, :dec1.size(2), :dec1.size(3)]\n",
    "        dec1 = self.decoder1(torch.cat([dec1, enc1_cropped], dim=1))\n",
    "\n",
    "        out = self.final(dec1)\n",
    "        out = F.pad(out, (0, 181 - out.shape[-1], 0, 50 - out.shape[-2]))\n",
    "        return out\n",
    "'''\n",
    "# Write code to file\n",
    "with open('submission_model.py', 'w',encoding=\"utf-8\") as f:\n",
    "    f.write(model_code)\n",
    "print(\"submission_model.py file has been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3438e-8176-4686-9244-4661e795cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parameters of the model\n",
    "torch.save(model.state_dict(), 'submission_dic.pth')\n",
    "print(\"submission_dic.pth file has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45d29b-a901-475b-9664-d9cbb5f52a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block mainly specifies the submission format of this question.\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the files to zip and the zip file name.\n",
    "files_to_zip = ['submission_model.py', 'submission_dic.pth']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "# Create a zip file\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        # Add the file to the zip fil\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f'{zip_filename} Created successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
